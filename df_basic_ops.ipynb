{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('filter_ops').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape :  (1762, 7)\n",
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data/appl_stock.csv',inferSchema=True,header = True)\n",
    "print('DataFrame shape : ',(df.count(),len(df.columns)))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "|summary|      Date|              Open|              High|               Low|            Close|             Volume|         Adj Close|\n",
      "+-------+----------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "|  count|      1762|              1762|              1762|              1762|             1762|               1762|              1762|\n",
      "|   mean|      null| 313.0763111589103| 315.9112880164581| 309.8282405079457|312.9270656379113|9.422577587968218E7| 75.00174115607275|\n",
      "| stddev|      null|185.29946803981522|186.89817686485767|183.38391664371008|185.1471036170943|6.020518776592709E7| 28.57492972179906|\n",
      "|    min|2010-01-04|              90.0|         90.699997|         89.470001|        90.279999|           11475900|         24.881912|\n",
      "|    max|2016-12-30|        702.409988|        705.070023|        699.569977|       702.100021|          470249500|127.96609099999999|\n",
      "+-------+----------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+----------+----------+---------+------------------+\n",
      "|      Date|      Open|  High|       Low|     Close|   Volume|         Adj Close|\n",
      "+----------+----------+------+----------+----------+---------+------------------+\n",
      "|2010-01-06|214.379993|215.23|210.750004|210.969995|138040000|27.333178000000004|\n",
      "+----------+----------+------+----------+----------+---------+------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "############## Filtering ##################\n",
    "\n",
    "# 1) SQL Syntax\n",
    "# print(df.filter('Close < 500').show())\n",
    "# print(df.filter('Close < 500 AND High > 200').show())\n",
    "# print(df.filter('High = 215.23').show())\n",
    "\n",
    "# 2) Python Syntax\n",
    "# print(df.filter(df['Close'] < 500).show())\n",
    "# print(df.filter( (df['Close'] < 500) & (df['High'] > 200) ).show())\n",
    "print(df.filter(df['High'] == 215.23).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|  High|       Low|\n",
      "+------+----------+\n",
      "|215.23|210.750004|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############## Select Specific Columns\n",
    "df.filter(df['High'] == 215.23).select(['High','Low']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Date='2010-01-06', Open=214.379993, High=215.23, Low=210.750004, Close=210.969995, Volume=138040000, Adj Close=27.333178000000004)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Date': '2010-01-06',\n",
       " 'Open': 214.379993,\n",
       " 'High': 215.23,\n",
       " 'Low': 210.750004,\n",
       " 'Close': 210.969995,\n",
       " 'Volume': 138040000,\n",
       " 'Adj Close': 27.333178000000004}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############# Getting row as dictionary\n",
    "res = df.filter(df['High'] == 215.23).collect()\n",
    "print(res)\n",
    "res[0].asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|Year|Average Open|\n",
      "+----+------------+\n",
      "|2010|      259.96|\n",
      "|2011|      364.06|\n",
      "|2012|      576.65|\n",
      "|2013|      473.13|\n",
      "|2014|      295.14|\n",
      "|2015|      120.18|\n",
      "|2016|      104.51|\n",
      "+----+------------+\n",
      "\n",
      "+----+------------+\n",
      "|Year|Average High|\n",
      "+----+------------+\n",
      "|2010|      262.37|\n",
      "|2011|      367.42|\n",
      "|2012|      581.83|\n",
      "|2013|      477.64|\n",
      "|2014|      297.56|\n",
      "|2015|      121.24|\n",
      "|2016|      105.43|\n",
      "+----+------------+\n",
      "\n",
      "+----+-----------+\n",
      "|Year|Average Low|\n",
      "+----+-----------+\n",
      "|2010|     256.85|\n",
      "|2011|     360.30|\n",
      "|2012|     569.92|\n",
      "|2013|     468.25|\n",
      "|2014|     292.99|\n",
      "|2015|     118.86|\n",
      "|2016|     103.69|\n",
      "+----+-----------+\n",
      "\n",
      "+----+-------------+\n",
      "|Year|Average Close|\n",
      "+----+-------------+\n",
      "|2010|       259.84|\n",
      "|2011|       364.00|\n",
      "|2012|       576.05|\n",
      "|2013|       472.63|\n",
      "|2014|       295.40|\n",
      "|2015|       120.04|\n",
      "|2016|       104.60|\n",
      "+----+-------------+\n",
      "\n",
      "+----+--------------+\n",
      "|Year|Average Volume|\n",
      "+----+--------------+\n",
      "|2010|149,826,316.67|\n",
      "|2011|123,074,741.67|\n",
      "|2012|131,964,204.40|\n",
      "|2013|101,608,700.00|\n",
      "|2014| 63,152,730.56|\n",
      "|2015| 51,837,886.90|\n",
      "|2016| 38,415,362.30|\n",
      "+----+--------------+\n",
      "\n",
      "+----+-----------------+\n",
      "|Year|Average Adj Close|\n",
      "+----+-----------------+\n",
      "|2010|            33.67|\n",
      "|2011|            47.16|\n",
      "|2012|            74.81|\n",
      "|2013|            62.62|\n",
      "|2014|            87.64|\n",
      "|2015|           115.97|\n",
      "|2016|           103.15|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########## Computing average prices per year ############\n",
    "\n",
    "from pyspark.sql.functions import year,mean,format_number\n",
    "\n",
    "cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close'] \n",
    "df_new = df.withColumn('Year',year(df['Date']))\n",
    "group = df_new.groupBy('Year').mean().orderBy('Year')\n",
    "for col in cols:\n",
    "    col_name = 'avg({})'.format(col) \n",
    "    group.select(['Year',format_number(col_name,2).alias('Average {}'.format(col))]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUP_BY AND AGG methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('group_ops').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data/sales_info.csv',inferSchema=True,header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.group.GroupedData object at 0x7f7855485710>\n"
     ]
    }
   ],
   "source": [
    "group_company = df.groupBy('Company')\n",
    "print(group_company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+\n",
      "|Company|max(Sales)|min(Person)|\n",
      "+-------+----------+-----------+\n",
      "|   APPL|     750.0|      Chris|\n",
      "|   GOOG|     340.0|    Charlie|\n",
      "|     FB|     870.0|       Carl|\n",
      "|   MSFT|     600.0|        Amy|\n",
      "+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_company.agg({'Person':'min','Sales':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|max(Sales)|min(Person)|\n",
      "+----------+-----------+\n",
      "|     870.0|      Chris|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.agg({'Sales':'mean'}).show()\n",
    "df.agg({\n",
    "    'Person':'min',\n",
    "    'Sales':'max'\n",
    "}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   std|\n",
      "+------+\n",
      "|250.09|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############### Functions ##############\n",
    "from pyspark.sql.functions import countDistinct,format_number,stddev\n",
    "\n",
    "df.select(format_number(stddev('Sales'),2).alias('std')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################ Sorting ###################\n",
    "# df.orderBy('Sales').show() # : ascending ordering\n",
    "df.orderBy(df['Sales'].desc()).show() # descending ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
